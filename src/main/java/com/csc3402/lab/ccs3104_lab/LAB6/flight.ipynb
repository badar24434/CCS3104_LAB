{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc8800b",
   "metadata": {},
   "source": [
    "# ‚úàÔ∏è Flight Operations Data Engineering Pipeline\n",
    "## AviationStack + OpenSky Network Integration\n",
    "\n",
    "This pipeline demonstrates **real-world data engineering skills** with heavy SQL usage:\n",
    "- **Extract**: Live flight data from OpenSky Network API\n",
    "- **Transform**: PySpark + Spark SQL for data processing\n",
    "- **Load**: Star schema data warehouse design\n",
    "- **Analyze**: SQL-based analytics and KPIs\n",
    "\n",
    "### Why SQL Matters in Data Engineering\n",
    "- 70%+ of data engineering work involves SQL\n",
    "- Most data warehouses (Snowflake, BigQuery, Redshift) are SQL-based\n",
    "- ETL/ELT pipelines heavily use SQL transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Installation\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q pyspark requests pandas plotly\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b56904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize Spark Session with SQL Support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightOperationsPipeline\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"üöÄ Spark version: {spark.version}\")\n",
    "print(\"‚úÖ Spark SQL session initialized!\")\n",
    "print(\"\\nüìä SQL is enabled - we'll use Spark SQL extensively in this pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: API Configuration\n",
    "# OpenSky Network API - FREE, no key required for basic usage\n",
    "OPENSKY_BASE_URL = \"https://opensky-network.org/api\"\n",
    "\n",
    "# Define major airports for analysis (ICAO codes)\n",
    "AIRPORTS = {\n",
    "    'KJFK': {'name': 'John F. Kennedy International', 'city': 'New York', 'country': 'US', 'lat': 40.6413, 'lon': -73.7781},\n",
    "    'KLAX': {'name': 'Los Angeles International', 'city': 'Los Angeles', 'country': 'US', 'lat': 33.9425, 'lon': -118.4081},\n",
    "    'EGLL': {'name': 'London Heathrow', 'city': 'London', 'country': 'UK', 'lat': 51.4700, 'lon': -0.4543},\n",
    "    'LFPG': {'name': 'Paris Charles de Gaulle', 'city': 'Paris', 'country': 'FR', 'lat': 49.0097, 'lon': 2.5479},\n",
    "    'RJTT': {'name': 'Tokyo Haneda', 'city': 'Tokyo', 'country': 'JP', 'lat': 35.5494, 'lon': 139.7798},\n",
    "    'WSSS': {'name': 'Singapore Changi', 'city': 'Singapore', 'country': 'SG', 'lat': 1.3644, 'lon': 103.9915},\n",
    "    'OMDB': {'name': 'Dubai International', 'city': 'Dubai', 'country': 'AE', 'lat': 25.2528, 'lon': 55.3644},\n",
    "    'VHHH': {'name': 'Hong Kong International', 'city': 'Hong Kong', 'country': 'HK', 'lat': 22.3080, 'lon': 113.9185},\n",
    "    'EDDF': {'name': 'Frankfurt Airport', 'city': 'Frankfurt', 'country': 'DE', 'lat': 50.0379, 'lon': 8.5622},\n",
    "    'YSSY': {'name': 'Sydney Kingsford Smith', 'city': 'Sydney', 'country': 'AU', 'lat': -33.9399, 'lon': 151.1753}\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configured {len(AIRPORTS)} major international airports for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977fc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: EXTRACT - Fetch Live Flight Data from OpenSky Network\n",
    "def extract_flight_states():\n",
    "    \"\"\"Extract current flight states from OpenSky Network API\"\"\"\n",
    "    \n",
    "    url = f\"{OPENSKY_BASE_URL}/states/all\"\n",
    "    \n",
    "    print(\"üì° Fetching live flight data from OpenSky Network...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        states = data.get('states', [])\n",
    "        timestamp = data.get('time', int(datetime.now().timestamp()))\n",
    "        \n",
    "        print(f\"‚úÖ Retrieved {len(states)} aircraft positions\")\n",
    "        \n",
    "        # Parse flight states into structured format\n",
    "        flights = []\n",
    "        for state in states[:500]:  # Limit for demo (API returns thousands)\n",
    "            if state[1]:  # Has callsign\n",
    "                flight = {\n",
    "                    'icao24': state[0],\n",
    "                    'callsign': state[1].strip() if state[1] else None,\n",
    "                    'origin_country': state[2],\n",
    "                    'longitude': float(state[5]) if state[5] else None,\n",
    "                    'latitude': float(state[6]) if state[6] else None,\n",
    "                    'baro_altitude': float(state[7]) if state[7] else None,\n",
    "                    'on_ground': bool(state[8]),\n",
    "                    'velocity': float(state[9]) if state[9] else None,\n",
    "                    'true_track': float(state[10]) if state[10] else None,\n",
    "                    'vertical_rate': float(state[11]) if state[11] else None,\n",
    "                    'geo_altitude': float(state[13]) if state[13] else None,\n",
    "                    'squawk': state[14],\n",
    "                    'position_source': int(state[16]) if state[16] else 0,\n",
    "                    'timestamp': timestamp,\n",
    "                    'extraction_time': datetime.now().isoformat()\n",
    "                }\n",
    "                flights.append(flight)\n",
    "        \n",
    "        return flights, timestamp\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API Error: {str(e)}\")\n",
    "        return [], None\n",
    "\n",
    "# Execute extraction\n",
    "raw_flights, api_timestamp = extract_flight_states()\n",
    "\n",
    "if raw_flights:\n",
    "    print(f\"\\nüìã Sample flight data:\")\n",
    "    print(json.dumps(raw_flights[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f737e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Generate Simulated Historical Flight Data\n",
    "# (Supplements API data for comprehensive warehouse demo)\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_historical_flights(num_records=1000):\n",
    "    \"\"\"Generate realistic historical flight data for warehouse demo\"\"\"\n",
    "    \n",
    "    airlines = {\n",
    "        'AA': 'American Airlines', 'UA': 'United Airlines', 'DL': 'Delta Air Lines',\n",
    "        'BA': 'British Airways', 'LH': 'Lufthansa', 'AF': 'Air France',\n",
    "        'EK': 'Emirates', 'SQ': 'Singapore Airlines', 'QF': 'Qantas',\n",
    "        'JL': 'Japan Airlines', 'CX': 'Cathay Pacific', 'NH': 'All Nippon Airways'\n",
    "    }\n",
    "    \n",
    "    aircraft_types = ['B737', 'B777', 'B787', 'A320', 'A350', 'A380']\n",
    "    airport_codes = list(AIRPORTS.keys())\n",
    "    \n",
    "    flights = []\n",
    "    base_date = datetime.now() - timedelta(days=30)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        airline_code = random.choice(list(airlines.keys()))\n",
    "        flight_num = f\"{airline_code}{random.randint(100, 9999)}\"\n",
    "        \n",
    "        origin = random.choice(airport_codes)\n",
    "        dest = random.choice([a for a in airport_codes if a != origin])\n",
    "        \n",
    "        # Scheduled times\n",
    "        sched_dep = base_date + timedelta(\n",
    "            days=random.randint(0, 30),\n",
    "            hours=random.randint(0, 23),\n",
    "            minutes=random.choice([0, 15, 30, 45])\n",
    "        )\n",
    "        \n",
    "        # Flight duration based on distance (simplified)\n",
    "        flight_duration = random.randint(60, 840)  # 1-14 hours in minutes\n",
    "        sched_arr = sched_dep + timedelta(minutes=flight_duration)\n",
    "        \n",
    "        # Actual times with delays\n",
    "        dep_delay = random.choices(\n",
    "            [0, random.randint(1, 15), random.randint(16, 60), random.randint(61, 180)],\n",
    "            weights=[0.6, 0.25, 0.1, 0.05]\n",
    "        )[0]\n",
    "        arr_delay = dep_delay + random.randint(-10, 20)\n",
    "        \n",
    "        actual_dep = sched_dep + timedelta(minutes=dep_delay)\n",
    "        actual_arr = sched_arr + timedelta(minutes=max(0, arr_delay))\n",
    "        \n",
    "        # Flight status\n",
    "        if arr_delay <= 0:\n",
    "            status = 'On Time'\n",
    "        elif arr_delay <= 15:\n",
    "            status = 'Slight Delay'\n",
    "        elif arr_delay <= 60:\n",
    "            status = 'Delayed'\n",
    "        else:\n",
    "            status = 'Significantly Delayed'\n",
    "        \n",
    "        flight = {\n",
    "            'flight_id': f\"FL{i+1:06d}\",\n",
    "            'flight_number': flight_num,\n",
    "            'airline_code': airline_code,\n",
    "            'airline_name': airlines[airline_code],\n",
    "            'aircraft_type': random.choice(aircraft_types),\n",
    "            'origin_airport': origin,\n",
    "            'destination_airport': dest,\n",
    "            'scheduled_departure': sched_dep.isoformat(),\n",
    "            'scheduled_arrival': sched_arr.isoformat(),\n",
    "            'actual_departure': actual_dep.isoformat(),\n",
    "            'actual_arrival': actual_arr.isoformat(),\n",
    "            'departure_delay_minutes': dep_delay,\n",
    "            'arrival_delay_minutes': max(0, arr_delay),\n",
    "            'flight_status': status,\n",
    "            'flight_duration_minutes': flight_duration,\n",
    "            'passengers': random.randint(50, 400),\n",
    "            'load_factor': round(random.uniform(0.55, 0.98), 2),\n",
    "            'cancelled': random.random() < 0.02,  # 2% cancellation rate\n",
    "            'diverted': random.random() < 0.01    # 1% diversion rate\n",
    "        }\n",
    "        flights.append(flight)\n",
    "    \n",
    "    return flights\n",
    "\n",
    "# Generate historical data\n",
    "historical_flights = generate_historical_flights(1000)\n",
    "print(f\"‚úÖ Generated {len(historical_flights)} historical flight records\")\n",
    "print(f\"\\nüìã Sample historical flight:\")\n",
    "print(json.dumps(historical_flights[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: LOAD - Create Raw Layer DataFrames and Register as SQL Tables\n",
    "# =========================================================================\n",
    "# This is the RAW layer - data as extracted, minimal transformations\n",
    "\n",
    "# Create airport dimension DataFrame\n",
    "airport_data = [\n",
    "    {'airport_code': code, **details} \n",
    "    for code, details in AIRPORTS.items()\n",
    "]\n",
    "df_airports_raw = spark.createDataFrame(airport_data)\n",
    "\n",
    "# Create flights DataFrame\n",
    "df_flights_raw = spark.createDataFrame(historical_flights)\n",
    "\n",
    "# Create live positions DataFrame (if API call succeeded)\n",
    "if raw_flights:\n",
    "    df_positions_raw = spark.createDataFrame(raw_flights)\n",
    "else:\n",
    "    # Create empty DataFrame with schema\n",
    "    df_positions_raw = spark.createDataFrame([], schema=StructType([\n",
    "        StructField(\"icao24\", StringType()),\n",
    "        StructField(\"callsign\", StringType()),\n",
    "        StructField(\"origin_country\", StringType()),\n",
    "        StructField(\"latitude\", DoubleType()),\n",
    "        StructField(\"longitude\", DoubleType())\n",
    "    ]))\n",
    "\n",
    "# ‚≠ê REGISTER AS SQL TEMPORARY VIEWS - This enables SQL queries!\n",
    "df_airports_raw.createOrReplaceTempView(\"raw_airports\")\n",
    "df_flights_raw.createOrReplaceTempView(\"raw_flights\")\n",
    "df_positions_raw.createOrReplaceTempView(\"raw_positions\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶ RAW LAYER - Data Loaded and Registered as SQL Tables\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ raw_airports: {df_airports_raw.count()} records\")\n",
    "print(f\"‚úÖ raw_flights: {df_flights_raw.count()} records\")\n",
    "print(f\"‚úÖ raw_positions: {df_positions_raw.count()} records\")\n",
    "print(\"\\nüîß Tables available for SQL queries:\")\n",
    "print(\"   - raw_airports\")\n",
    "print(\"   - raw_flights\")\n",
    "print(\"   - raw_positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b67c4",
   "metadata": {},
   "source": [
    "## üî∑ SQL Section Begins Here\n",
    "From this point forward, we'll heavily use **Spark SQL** for data transformations and analytics.\n",
    "This mirrors real-world data engineering where SQL is the primary language for:\n",
    "- Data transformations (ETL/ELT)\n",
    "- Creating dimension and fact tables\n",
    "- Business logic implementation\n",
    "- Analytics and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbe44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: SQL - Explore Raw Data Layer\n",
    "# =========================================================================\n",
    "# Using SQL to understand our raw data before transformation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç EXPLORING RAW DATA WITH SQL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query 1: Preview raw flights\n",
    "print(\"\\nüìã Query 1: Preview Raw Flights Table\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        flight_number,\n",
    "        airline_name,\n",
    "        origin_airport,\n",
    "        destination_airport,\n",
    "        flight_status,\n",
    "        departure_delay_minutes\n",
    "    FROM raw_flights\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Query 2: Check data quality - null counts\n",
    "print(\"\\nüìã Query 2: Data Quality Check - Null Values\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        SUM(CASE WHEN flight_number IS NULL THEN 1 ELSE 0 END) as null_flight_numbers,\n",
    "        SUM(CASE WHEN origin_airport IS NULL THEN 1 ELSE 0 END) as null_origins,\n",
    "        SUM(CASE WHEN destination_airport IS NULL THEN 1 ELSE 0 END) as null_destinations,\n",
    "        SUM(CASE WHEN cancelled = true THEN 1 ELSE 0 END) as cancelled_flights\n",
    "    FROM raw_flights\n",
    "\"\"\").show()\n",
    "\n",
    "# Query 3: Preview airports\n",
    "print(\"\\nüìã Query 3: Airport Reference Data\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        airport_code,\n",
    "        name,\n",
    "        city,\n",
    "        country,\n",
    "        ROUND(lat, 2) as latitude,\n",
    "        ROUND(lon, 2) as longitude\n",
    "    FROM raw_airports\n",
    "    ORDER BY country, city\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f021cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: SQL - Create CURATED LAYER (Dimension Tables)\n",
    "# =========================================================================\n",
    "# Transform raw data into clean, business-ready dimension tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üèóÔ∏è CREATING CURATED LAYER - DIMENSION TABLES (SQL)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# DIM_AIRPORT - Airport Dimension\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW dim_airport AS\n",
    "    SELECT \n",
    "        ROW_NUMBER() OVER (ORDER BY airport_code) as airport_key,\n",
    "        airport_code,\n",
    "        name as airport_name,\n",
    "        city,\n",
    "        country,\n",
    "        lat as latitude,\n",
    "        lon as longitude,\n",
    "        CASE \n",
    "            WHEN country IN ('US', 'CA', 'MX') THEN 'North America'\n",
    "            WHEN country IN ('UK', 'FR', 'DE') THEN 'Europe'\n",
    "            WHEN country IN ('JP', 'SG', 'HK') THEN 'Asia Pacific'\n",
    "            WHEN country IN ('AE') THEN 'Middle East'\n",
    "            WHEN country IN ('AU') THEN 'Oceania'\n",
    "            ELSE 'Other'\n",
    "        END as region,\n",
    "        CURRENT_TIMESTAMP() as created_at,\n",
    "        CURRENT_TIMESTAMP() as updated_at\n",
    "    FROM raw_airports\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ dim_airport created:\")\n",
    "spark.sql(\"SELECT * FROM dim_airport\").show(truncate=False)\n",
    "\n",
    "# DIM_AIRLINE - Airline Dimension (extracted from flights)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW dim_airline AS\n",
    "    SELECT DISTINCT\n",
    "        ROW_NUMBER() OVER (ORDER BY airline_code) as airline_key,\n",
    "        airline_code,\n",
    "        airline_name,\n",
    "        CASE \n",
    "            WHEN airline_code IN ('AA', 'UA', 'DL') THEN 'US Carrier'\n",
    "            WHEN airline_code IN ('BA', 'LH', 'AF') THEN 'European Carrier'\n",
    "            WHEN airline_code IN ('EK') THEN 'Middle East Carrier'\n",
    "            WHEN airline_code IN ('SQ', 'CX', 'JL', 'NH') THEN 'Asian Carrier'\n",
    "            WHEN airline_code IN ('QF') THEN 'Oceania Carrier'\n",
    "            ELSE 'Other'\n",
    "        END as carrier_type,\n",
    "        CURRENT_TIMESTAMP() as created_at\n",
    "    FROM raw_flights\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ dim_airline created:\")\n",
    "spark.sql(\"SELECT * FROM dim_airline\").show(truncate=False)\n",
    "\n",
    "# DIM_AIRCRAFT - Aircraft Dimension\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW dim_aircraft AS\n",
    "    SELECT DISTINCT\n",
    "        ROW_NUMBER() OVER (ORDER BY aircraft_type) as aircraft_key,\n",
    "        aircraft_type,\n",
    "        CASE \n",
    "            WHEN aircraft_type LIKE 'B%' THEN 'Boeing'\n",
    "            WHEN aircraft_type LIKE 'A%' THEN 'Airbus'\n",
    "            ELSE 'Other'\n",
    "        END as manufacturer,\n",
    "        CASE \n",
    "            WHEN aircraft_type IN ('A320', 'B737') THEN 'Narrow Body'\n",
    "            WHEN aircraft_type IN ('A350', 'B777', 'B787') THEN 'Wide Body'\n",
    "            WHEN aircraft_type IN ('A380') THEN 'Super Jumbo'\n",
    "            ELSE 'Unknown'\n",
    "        END as aircraft_category,\n",
    "        CASE \n",
    "            WHEN aircraft_type IN ('A320', 'B737') THEN 180\n",
    "            WHEN aircraft_type IN ('A350') THEN 300\n",
    "            WHEN aircraft_type IN ('B777') THEN 350\n",
    "            WHEN aircraft_type IN ('B787') THEN 290\n",
    "            WHEN aircraft_type IN ('A380') THEN 500\n",
    "            ELSE 200\n",
    "        END as typical_capacity\n",
    "    FROM raw_flights\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ dim_aircraft created:\")\n",
    "spark.sql(\"SELECT * FROM dim_aircraft\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf294bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: SQL - Create DIM_DATE (Date Dimension - Critical for any Data Warehouse!)\n",
    "# =========================================================================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW dim_date AS\n",
    "    WITH date_range AS (\n",
    "        SELECT DISTINCT \n",
    "            DATE(scheduled_departure) as flight_date\n",
    "        FROM raw_flights\n",
    "    )\n",
    "    SELECT \n",
    "        ROW_NUMBER() OVER (ORDER BY flight_date) as date_key,\n",
    "        flight_date as full_date,\n",
    "        YEAR(flight_date) as year,\n",
    "        MONTH(flight_date) as month,\n",
    "        DAY(flight_date) as day,\n",
    "        QUARTER(flight_date) as quarter,\n",
    "        WEEKOFYEAR(flight_date) as week_of_year,\n",
    "        DAYOFWEEK(flight_date) as day_of_week,\n",
    "        DATE_FORMAT(flight_date, 'EEEE') as day_name,\n",
    "        DATE_FORMAT(flight_date, 'MMMM') as month_name,\n",
    "        CASE \n",
    "            WHEN DAYOFWEEK(flight_date) IN (1, 7) THEN TRUE \n",
    "            ELSE FALSE \n",
    "        END as is_weekend,\n",
    "        CASE \n",
    "            WHEN MONTH(flight_date) IN (12, 1, 2) THEN 'Winter'\n",
    "            WHEN MONTH(flight_date) IN (3, 4, 5) THEN 'Spring'\n",
    "            WHEN MONTH(flight_date) IN (6, 7, 8) THEN 'Summer'\n",
    "            ELSE 'Fall'\n",
    "        END as season\n",
    "    FROM date_range\n",
    "    ORDER BY flight_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÖ DIM_DATE - Date Dimension Created\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"SELECT * FROM dim_date LIMIT 10\").show()\n",
    "\n",
    "# DIM_TIME - Time Dimension\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW dim_time AS\n",
    "    SELECT \n",
    "        hour as time_key,\n",
    "        hour,\n",
    "        CASE \n",
    "            WHEN hour = 0 THEN 12\n",
    "            WHEN hour <= 12 THEN hour\n",
    "            ELSE hour - 12\n",
    "        END as hour_12,\n",
    "        CASE WHEN hour < 12 THEN 'AM' ELSE 'PM' END as am_pm,\n",
    "        CASE \n",
    "            WHEN hour BETWEEN 5 AND 11 THEN 'Morning'\n",
    "            WHEN hour BETWEEN 12 AND 16 THEN 'Afternoon'\n",
    "            WHEN hour BETWEEN 17 AND 20 THEN 'Evening'\n",
    "            ELSE 'Night'\n",
    "        END as time_of_day,\n",
    "        CASE \n",
    "            WHEN hour BETWEEN 6 AND 9 THEN TRUE\n",
    "            WHEN hour BETWEEN 17 AND 19 THEN TRUE\n",
    "            ELSE FALSE\n",
    "        END as is_peak_hour\n",
    "    FROM (SELECT DISTINCT HOUR(scheduled_departure) as hour FROM raw_flights)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚è∞ DIM_TIME - Time Dimension Created\")\n",
    "spark.sql(\"SELECT * FROM dim_time ORDER BY hour\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b873039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: SQL - Create FACT Tables (Star Schema Core)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚≠ê CREATING FACT TABLES - Star Schema Core\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# FACT_FLIGHTS - Main fact table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW fact_flights AS\n",
    "    SELECT \n",
    "        f.flight_id as flight_key,\n",
    "        al.airline_key,\n",
    "        ac.aircraft_key,\n",
    "        orig.airport_key as origin_airport_key,\n",
    "        dest.airport_key as destination_airport_key,\n",
    "        d.date_key as departure_date_key,\n",
    "        t.time_key as departure_time_key,\n",
    "        \n",
    "        -- Measures (Facts)\n",
    "        f.departure_delay_minutes,\n",
    "        f.arrival_delay_minutes,\n",
    "        f.flight_duration_minutes,\n",
    "        f.passengers,\n",
    "        f.load_factor,\n",
    "        \n",
    "        -- Degenerate dimensions\n",
    "        f.flight_number,\n",
    "        f.flight_status,\n",
    "        f.cancelled,\n",
    "        f.diverted,\n",
    "        \n",
    "        -- Timestamps\n",
    "        f.scheduled_departure,\n",
    "        f.scheduled_arrival,\n",
    "        f.actual_departure,\n",
    "        f.actual_arrival\n",
    "        \n",
    "    FROM raw_flights f\n",
    "    LEFT JOIN dim_airline al ON f.airline_code = al.airline_code\n",
    "    LEFT JOIN dim_aircraft ac ON f.aircraft_type = ac.aircraft_type\n",
    "    LEFT JOIN dim_airport orig ON f.origin_airport = orig.airport_code\n",
    "    LEFT JOIN dim_airport dest ON f.destination_airport = dest.airport_code\n",
    "    LEFT JOIN dim_date d ON DATE(f.scheduled_departure) = d.full_date\n",
    "    LEFT JOIN dim_time t ON HOUR(f.scheduled_departure) = t.hour\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ fact_flights created\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        flight_key, airline_key, origin_airport_key, destination_airport_key,\n",
    "        departure_delay_minutes, flight_status\n",
    "    FROM fact_flights \n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "# FACT_DELAYS - Aggregated delay fact table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW fact_delays AS\n",
    "    SELECT \n",
    "        d.date_key,\n",
    "        al.airline_key,\n",
    "        orig.airport_key as airport_key,\n",
    "        \n",
    "        -- Delay metrics\n",
    "        COUNT(*) as total_flights,\n",
    "        SUM(CASE WHEN f.departure_delay_minutes > 0 THEN 1 ELSE 0 END) as delayed_flights,\n",
    "        SUM(CASE WHEN f.departure_delay_minutes > 15 THEN 1 ELSE 0 END) as significantly_delayed,\n",
    "        ROUND(AVG(f.departure_delay_minutes), 2) as avg_delay_minutes,\n",
    "        MAX(f.departure_delay_minutes) as max_delay_minutes,\n",
    "        SUM(CASE WHEN f.cancelled THEN 1 ELSE 0 END) as cancelled_flights,\n",
    "        \n",
    "        -- On-time performance\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN f.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), \n",
    "            2\n",
    "        ) as on_time_performance_pct\n",
    "        \n",
    "    FROM raw_flights f\n",
    "    LEFT JOIN dim_airline al ON f.airline_code = al.airline_code\n",
    "    LEFT JOIN dim_airport orig ON f.origin_airport = orig.airport_code\n",
    "    LEFT JOIN dim_date d ON DATE(f.scheduled_departure) = d.full_date\n",
    "    GROUP BY d.date_key, al.airline_key, orig.airport_key\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ fact_delays created\")\n",
    "spark.sql(\"SELECT * FROM fact_delays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: SQL - Create FACT_ROUTES Table\n",
    "# =========================================================================\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW fact_routes AS\n",
    "    SELECT \n",
    "        orig.airport_key as origin_airport_key,\n",
    "        dest.airport_key as destination_airport_key,\n",
    "        al.airline_key,\n",
    "        \n",
    "        -- Route metrics\n",
    "        COUNT(*) as total_flights,\n",
    "        COUNT(DISTINCT f.flight_number) as unique_flight_numbers,\n",
    "        ROUND(AVG(f.flight_duration_minutes), 0) as avg_flight_duration,\n",
    "        ROUND(AVG(f.passengers), 0) as avg_passengers,\n",
    "        ROUND(AVG(f.load_factor) * 100, 1) as avg_load_factor_pct,\n",
    "        \n",
    "        -- Delay analysis by route\n",
    "        ROUND(AVG(f.departure_delay_minutes), 2) as avg_departure_delay,\n",
    "        ROUND(AVG(f.arrival_delay_minutes), 2) as avg_arrival_delay,\n",
    "        \n",
    "        -- Performance\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN f.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            2\n",
    "        ) as route_on_time_pct,\n",
    "        \n",
    "        -- Reliability\n",
    "        SUM(CASE WHEN f.cancelled THEN 1 ELSE 0 END) as cancelled_flights,\n",
    "        SUM(CASE WHEN f.diverted THEN 1 ELSE 0 END) as diverted_flights\n",
    "        \n",
    "    FROM raw_flights f\n",
    "    LEFT JOIN dim_airline al ON f.airline_code = al.airline_code\n",
    "    LEFT JOIN dim_airport orig ON f.origin_airport = orig.airport_code\n",
    "    LEFT JOIN dim_airport dest ON f.destination_airport = dest.airport_code\n",
    "    GROUP BY orig.airport_key, dest.airport_key, al.airline_key\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üõ´ FACT_ROUTES - Route Analysis Fact Table\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        origin_airport_key,\n",
    "        destination_airport_key,\n",
    "        total_flights,\n",
    "        avg_flight_duration,\n",
    "        route_on_time_pct\n",
    "    FROM fact_routes \n",
    "    ORDER BY total_flights DESC\n",
    "    LIMIT 15\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19163cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: SQL Analytics - On-Time Performance Analysis\n",
    "# =========================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä SQL ANALYTICS - ON-TIME PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query: Airline On-Time Performance Ranking\n",
    "print(\"\\nüèÜ Airline On-Time Performance Ranking:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        al.airline_name,\n",
    "        al.carrier_type,\n",
    "        COUNT(*) as total_flights,\n",
    "        SUM(CASE WHEN ff.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) as on_time_flights,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN ff.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            2\n",
    "        ) as on_time_pct,\n",
    "        ROUND(AVG(ff.departure_delay_minutes), 2) as avg_delay_mins,\n",
    "        SUM(CASE WHEN ff.cancelled THEN 1 ELSE 0 END) as cancellations\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_airline al ON ff.airline_key = al.airline_key\n",
    "    GROUP BY al.airline_name, al.carrier_type\n",
    "    ORDER BY on_time_pct DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Query: Airport Congestion Analysis\n",
    "print(\"\\nüõ¨ Airport Departure Performance:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ap.airport_name,\n",
    "        ap.city,\n",
    "        ap.region,\n",
    "        COUNT(*) as departures,\n",
    "        ROUND(AVG(ff.departure_delay_minutes), 2) as avg_delay,\n",
    "        MAX(ff.departure_delay_minutes) as max_delay,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN ff.departure_delay_minutes > 30 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            1\n",
    "        ) as severe_delay_pct\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_airport ap ON ff.origin_airport_key = ap.airport_key\n",
    "    GROUP BY ap.airport_name, ap.city, ap.region\n",
    "    ORDER BY avg_delay DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: SQL Analytics - Route Congestion & Utilization\n",
    "# =========================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä SQL ANALYTICS - ROUTE CONGESTION & AIRCRAFT UTILIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query: Busiest Routes with full details\n",
    "print(\"\\n‚úàÔ∏è Top 10 Busiest Routes:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        orig.city as origin_city,\n",
    "        dest.city as destination_city,\n",
    "        al.airline_name,\n",
    "        fr.total_flights,\n",
    "        fr.avg_flight_duration as duration_mins,\n",
    "        fr.avg_passengers,\n",
    "        fr.avg_load_factor_pct as load_factor,\n",
    "        fr.route_on_time_pct as on_time_pct\n",
    "    FROM fact_routes fr\n",
    "    JOIN dim_airport orig ON fr.origin_airport_key = orig.airport_key\n",
    "    JOIN dim_airport dest ON fr.destination_airport_key = dest.airport_key\n",
    "    JOIN dim_airline al ON fr.airline_key = al.airline_key\n",
    "    ORDER BY fr.total_flights DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Query: Aircraft Utilization by Type\n",
    "print(\"\\nüõ©Ô∏è Aircraft Utilization Analysis:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ac.aircraft_type,\n",
    "        ac.manufacturer,\n",
    "        ac.aircraft_category,\n",
    "        ac.typical_capacity,\n",
    "        COUNT(*) as flights_operated,\n",
    "        ROUND(AVG(ff.passengers), 0) as avg_passengers,\n",
    "        ROUND(AVG(ff.load_factor) * 100, 1) as avg_load_factor,\n",
    "        ROUND(AVG(ff.flight_duration_minutes), 0) as avg_flight_time,\n",
    "        ROUND(SUM(ff.flight_duration_minutes) / 60.0, 0) as total_flight_hours\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_aircraft ac ON ff.aircraft_key = ac.aircraft_key\n",
    "    GROUP BY ac.aircraft_type, ac.manufacturer, ac.aircraft_category, ac.typical_capacity\n",
    "    ORDER BY flights_operated DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19469ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: SQL Analytics - Advanced Window Functions\n",
    "# =========================================================================\n",
    "# Window functions are a KEY SQL skill for data engineering!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä ADVANCED SQL - WINDOW FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query: Running totals and rankings\n",
    "print(\"\\nüìà Flight Volume Trends with Running Totals:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        d.full_date,\n",
    "        d.day_name,\n",
    "        COUNT(*) as daily_flights,\n",
    "        SUM(COUNT(*)) OVER (ORDER BY d.full_date) as cumulative_flights,\n",
    "        ROUND(AVG(COUNT(*)) OVER (\n",
    "            ORDER BY d.full_date \n",
    "            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "        ), 0) as rolling_7day_avg\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_date d ON ff.departure_date_key = d.date_key\n",
    "    GROUP BY d.full_date, d.day_name\n",
    "    ORDER BY d.full_date\n",
    "    LIMIT 15\n",
    "\"\"\").show()\n",
    "\n",
    "# Query: Airline performance ranking within regions\n",
    "print(\"\\nüèÖ Airline Rankings by Region (PARTITION BY):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        airline_name,\n",
    "        total_flights,\n",
    "        on_time_pct,\n",
    "        RANK() OVER (PARTITION BY region ORDER BY on_time_pct DESC) as regional_rank,\n",
    "        DENSE_RANK() OVER (ORDER BY on_time_pct DESC) as global_rank\n",
    "    FROM (\n",
    "        SELECT \n",
    "            ap.region,\n",
    "            al.airline_name,\n",
    "            COUNT(*) as total_flights,\n",
    "            ROUND(\n",
    "                SUM(CASE WHEN ff.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "                2\n",
    "            ) as on_time_pct\n",
    "        FROM fact_flights ff\n",
    "        JOIN dim_airline al ON ff.airline_key = al.airline_key\n",
    "        JOIN dim_airport ap ON ff.origin_airport_key = ap.airport_key\n",
    "        GROUP BY ap.region, al.airline_name\n",
    "    )\n",
    "    ORDER BY region, regional_rank\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58efb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: SQL Analytics - Time-Based Analysis (Peak Hours)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚è∞ SQL ANALYTICS - TIME-BASED PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query: Peak hour analysis\n",
    "print(\"\\nüìä Flight Volume by Time of Day:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        t.time_of_day,\n",
    "        t.is_peak_hour,\n",
    "        COUNT(*) as total_flights,\n",
    "        ROUND(AVG(ff.departure_delay_minutes), 2) as avg_delay,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN ff.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            2\n",
    "        ) as on_time_pct\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_time t ON ff.departure_time_key = t.time_key\n",
    "    GROUP BY t.time_of_day, t.is_peak_hour\n",
    "    ORDER BY \n",
    "        CASE t.time_of_day \n",
    "            WHEN 'Morning' THEN 1 \n",
    "            WHEN 'Afternoon' THEN 2 \n",
    "            WHEN 'Evening' THEN 3 \n",
    "            ELSE 4 \n",
    "        END\n",
    "\"\"\").show()\n",
    "\n",
    "# Query: Weekend vs Weekday performance\n",
    "print(\"\\nüìÖ Weekend vs Weekday Performance:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE WHEN d.is_weekend THEN 'Weekend' ELSE 'Weekday' END as day_type,\n",
    "        d.day_name,\n",
    "        COUNT(*) as flights,\n",
    "        ROUND(AVG(ff.departure_delay_minutes), 2) as avg_delay,\n",
    "        ROUND(AVG(ff.load_factor) * 100, 1) as avg_load_factor,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN ff.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            2\n",
    "        ) as on_time_pct\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_date d ON ff.departure_date_key = d.date_key\n",
    "    GROUP BY d.is_weekend, d.day_name, d.day_of_week\n",
    "    ORDER BY d.day_of_week\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: SQL - Create KPI Summary Views (Business Intelligence Layer)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä CREATING KPI SUMMARY VIEWS - BI LAYER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executive Dashboard KPIs\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW kpi_executive_summary AS\n",
    "    SELECT \n",
    "        -- Volume metrics\n",
    "        COUNT(*) as total_flights,\n",
    "        COUNT(DISTINCT airline_key) as active_airlines,\n",
    "        COUNT(DISTINCT origin_airport_key) as active_airports,\n",
    "        \n",
    "        -- Performance metrics\n",
    "        ROUND(AVG(departure_delay_minutes), 2) as avg_departure_delay,\n",
    "        ROUND(AVG(arrival_delay_minutes), 2) as avg_arrival_delay,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            2\n",
    "        ) as overall_on_time_pct,\n",
    "        \n",
    "        -- Capacity metrics\n",
    "        SUM(passengers) as total_passengers,\n",
    "        ROUND(AVG(load_factor) * 100, 1) as avg_load_factor,\n",
    "        \n",
    "        -- Reliability metrics\n",
    "        SUM(CASE WHEN cancelled THEN 1 ELSE 0 END) as total_cancellations,\n",
    "        ROUND(SUM(CASE WHEN cancelled THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as cancellation_rate,\n",
    "        \n",
    "        -- Operational hours\n",
    "        ROUND(SUM(flight_duration_minutes) / 60.0, 0) as total_flight_hours\n",
    "        \n",
    "    FROM fact_flights\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéØ Executive Summary KPIs:\")\n",
    "spark.sql(\"SELECT * FROM kpi_executive_summary\").show(vertical=True)\n",
    "\n",
    "# Airline Scorecard\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW kpi_airline_scorecard AS\n",
    "    SELECT \n",
    "        al.airline_name,\n",
    "        al.carrier_type,\n",
    "        COUNT(*) as flights,\n",
    "        ROUND(AVG(ff.departure_delay_minutes), 1) as avg_delay,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN ff.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            1\n",
    "        ) as on_time_pct,\n",
    "        ROUND(AVG(ff.load_factor) * 100, 1) as load_factor,\n",
    "        SUM(ff.passengers) as passengers,\n",
    "        ROUND(SUM(CASE WHEN ff.cancelled THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as cancel_rate\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_airline al ON ff.airline_key = al.airline_key\n",
    "    GROUP BY al.airline_name, al.carrier_type\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüè¢ Airline Scorecard:\")\n",
    "spark.sql(\"SELECT * FROM kpi_airline_scorecard ORDER BY on_time_pct DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ed151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Visualization - Airline Performance Dashboard\n",
    "# =========================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get data using SQL and convert to Pandas for visualization\n",
    "airline_perf = spark.sql(\"\"\"\n",
    "    SELECT * FROM kpi_airline_scorecard ORDER BY on_time_pct DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Create dashboard\n",
    "fig1 = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'On-Time Performance by Airline',\n",
    "        'Average Delay by Airline',\n",
    "        'Load Factor by Airline',\n",
    "        'Flights by Carrier Type'\n",
    "    ),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'pie'}]]\n",
    ")\n",
    "\n",
    "# On-Time Performance\n",
    "fig1.add_trace(\n",
    "    go.Bar(\n",
    "        x=airline_perf['airline_name'],\n",
    "        y=airline_perf['on_time_pct'],\n",
    "        marker_color='green',\n",
    "        name='On-Time %'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Average Delay\n",
    "fig1.add_trace(\n",
    "    go.Bar(\n",
    "        x=airline_perf['airline_name'],\n",
    "        y=airline_perf['avg_delay'],\n",
    "        marker_color='red',\n",
    "        name='Avg Delay'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Load Factor\n",
    "fig1.add_trace(\n",
    "    go.Bar(\n",
    "        x=airline_perf['airline_name'],\n",
    "        y=airline_perf['load_factor'],\n",
    "        marker_color='blue',\n",
    "        name='Load Factor'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Carrier Type Distribution\n",
    "carrier_counts = airline_perf.groupby('carrier_type')['flights'].sum()\n",
    "fig1.add_trace(\n",
    "    go.Pie(\n",
    "        labels=carrier_counts.index,\n",
    "        values=carrier_counts.values,\n",
    "        name='Carrier Type'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    height=700,\n",
    "    title_text=\"‚úàÔ∏è Airline Performance Dashboard\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig1.update_xaxes(tickangle=45)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Visualization - Route Map and Airport Analysis\n",
    "# =========================================================================\n",
    "\n",
    "# Get airport performance data\n",
    "airport_perf = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ap.airport_name,\n",
    "        ap.city,\n",
    "        ap.country,\n",
    "        ap.latitude,\n",
    "        ap.longitude,\n",
    "        ap.region,\n",
    "        COUNT(*) as total_flights,\n",
    "        ROUND(AVG(ff.departure_delay_minutes), 2) as avg_delay,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN ff.arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) * 100.0 / COUNT(*),\n",
    "            1\n",
    "        ) as on_time_pct\n",
    "    FROM fact_flights ff\n",
    "    JOIN dim_airport ap ON ff.origin_airport_key = ap.airport_key\n",
    "    GROUP BY ap.airport_name, ap.city, ap.country, ap.latitude, ap.longitude, ap.region\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Global Airport Map\n",
    "fig2 = px.scatter_geo(\n",
    "    airport_perf,\n",
    "    lat='latitude',\n",
    "    lon='longitude',\n",
    "    hover_name='city',\n",
    "    hover_data={\n",
    "        'airport_name': True,\n",
    "        'total_flights': True,\n",
    "        'avg_delay': ':.1f',\n",
    "        'on_time_pct': ':.1f',\n",
    "        'latitude': False,\n",
    "        'longitude': False\n",
    "    },\n",
    "    color='on_time_pct',\n",
    "    size='total_flights',\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    title='üåç Global Airport Performance Map',\n",
    "    size_max=30\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    geo=dict(\n",
    "        showland=True,\n",
    "        landcolor='lightgray',\n",
    "        showocean=True,\n",
    "        oceancolor='lightblue'\n",
    "    ),\n",
    "    height=500\n",
    ")\n",
    "fig2.show()\n",
    "\n",
    "# Delay by Region\n",
    "fig3 = px.bar(\n",
    "    airport_perf.groupby('region').agg({\n",
    "        'total_flights': 'sum',\n",
    "        'avg_delay': 'mean',\n",
    "        'on_time_pct': 'mean'\n",
    "    }).reset_index(),\n",
    "    x='region',\n",
    "    y='avg_delay',\n",
    "    color='on_time_pct',\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    title='üìä Average Delay by Region',\n",
    "    labels={'avg_delay': 'Avg Delay (mins)', 'on_time_pct': 'On-Time %'}\n",
    ")\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Export Data Warehouse to CSV Files\n",
    "# =========================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üíæ EXPORTING DATA WAREHOUSE TO FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export all dimension tables\n",
    "tables_to_export = [\n",
    "    ('dim_airport', 'dim_airport.csv'),\n",
    "    ('dim_airline', 'dim_airline.csv'),\n",
    "    ('dim_aircraft', 'dim_aircraft.csv'),\n",
    "    ('dim_date', 'dim_date.csv'),\n",
    "    ('dim_time', 'dim_time.csv'),\n",
    "    ('fact_flights', 'fact_flights.csv'),\n",
    "    ('fact_delays', 'fact_delays.csv'),\n",
    "    ('fact_routes', 'fact_routes.csv'),\n",
    "    ('kpi_executive_summary', 'kpi_executive_summary.csv'),\n",
    "    ('kpi_airline_scorecard', 'kpi_airline_scorecard.csv')\n",
    "]\n",
    "\n",
    "for table_name, filename in tables_to_export:\n",
    "    try:\n",
    "        df = spark.sql(f\"SELECT * FROM {table_name}\").toPandas()\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"‚úÖ {table_name} ‚Üí {filename} ({len(df)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting {table_name}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Data warehouse export complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Pipeline Summary and Data Warehouse Schema\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úàÔ∏è  FLIGHT OPERATIONS DATA ENGINEERING PIPELINE - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show all available SQL tables\n",
    "print(\"\\nüìã DATA WAREHOUSE SCHEMA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nüî∑ DIMENSION TABLES:\")\n",
    "print(\"   ‚Ä¢ dim_airport    - Airport reference data\")\n",
    "print(\"   ‚Ä¢ dim_airline    - Airline information\")\n",
    "print(\"   ‚Ä¢ dim_aircraft   - Aircraft types & capacity\")\n",
    "print(\"   ‚Ä¢ dim_date       - Date dimension (calendar)\")\n",
    "print(\"   ‚Ä¢ dim_time       - Time dimension (hours)\")\n",
    "\n",
    "print(\"\\n‚≠ê FACT TABLES:\")\n",
    "print(\"   ‚Ä¢ fact_flights   - Individual flight records\")\n",
    "print(\"   ‚Ä¢ fact_delays    - Aggregated delay metrics\")\n",
    "print(\"   ‚Ä¢ fact_routes    - Route performance summary\")\n",
    "\n",
    "print(\"\\nüìä KPI VIEWS:\")\n",
    "print(\"   ‚Ä¢ kpi_executive_summary  - High-level metrics\")\n",
    "print(\"   ‚Ä¢ kpi_airline_scorecard  - Airline performance\")\n",
    "\n",
    "# Get summary stats\n",
    "summary = spark.sql(\"SELECT * FROM kpi_executive_summary\").collect()[0]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"üìà KEY METRICS:\")\n",
    "print(f\"   ‚Ä¢ Total Flights: {summary['total_flights']:,}\")\n",
    "print(f\"   ‚Ä¢ Total Passengers: {summary['total_passengers']:,}\")\n",
    "print(f\"   ‚Ä¢ Active Airlines: {summary['active_airlines']}\")\n",
    "print(f\"   ‚Ä¢ Active Airports: {summary['active_airports']}\")\n",
    "print(f\"   ‚Ä¢ On-Time Performance: {summary['overall_on_time_pct']}%\")\n",
    "print(f\"   ‚Ä¢ Average Delay: {summary['avg_departure_delay']} mins\")\n",
    "print(f\"   ‚Ä¢ Total Flight Hours: {int(summary['total_flight_hours']):,}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"üîß SQL SKILLS DEMONSTRATED:\")\n",
    "print(\"   ‚úì CREATE VIEW / CREATE TABLE\")\n",
    "print(\"   ‚úì JOIN operations (LEFT, INNER)\")\n",
    "print(\"   ‚úì Aggregations (COUNT, SUM, AVG, MAX)\")\n",
    "print(\"   ‚úì CASE WHEN statements\")\n",
    "print(\"   ‚úì GROUP BY with multiple columns\")\n",
    "print(\"   ‚úì Window Functions (RANK, PARTITION BY, ROW_NUMBER)\")\n",
    "print(\"   ‚úì Subqueries and CTEs\")\n",
    "print(\"   ‚úì Date/Time functions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Pipeline executed successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7302ee",
   "metadata": {},
   "source": [
    "## üìö SQL Skills Summary for Data Engineering\n",
    "\n",
    "This pipeline demonstrates **essential SQL skills** used daily in data engineering:\n",
    "\n",
    "| SQL Concept | Usage in Pipeline | Real-World Application |\n",
    "|-------------|-------------------|----------------------|\n",
    "| **CREATE VIEW** | Dimension & Fact tables | Building data warehouse layers |\n",
    "| **JOIN** | Connecting facts to dimensions | Star schema queries |\n",
    "| **GROUP BY** | Aggregating metrics | KPI calculations |\n",
    "| **Window Functions** | RANK, PARTITION BY | Running totals, rankings |\n",
    "| **CASE WHEN** | Categorization logic | Business rules |\n",
    "| **Subqueries** | Complex aggregations | Multi-level analytics |\n",
    "| **CTEs** | Date dimension generation | Readable complex queries |\n",
    "\n",
    "### üéØ Why This Matters for Recruiters\n",
    "- **70%+ of data engineering work is SQL-based**\n",
    "- Shows understanding of **dimensional modeling** (Star Schema)\n",
    "- Demonstrates **ETL pipeline thinking** (Raw ‚Üí Curated ‚Üí BI layers)\n",
    "- Real **aviation/logistics domain knowledge**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
